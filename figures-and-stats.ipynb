{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy.stats import ttest_rel,ttest_ind\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('talk')\n",
    "%matplotlib inline\n",
    "plt.rcParams['font.size'] = 22\n",
    "\n",
    "%load_ext autoreload\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from humanUtils import *\n",
    "## this file contains most of the logic for loading and processing database\n",
    "from cswHumanDatabase import load_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-loading code version: RT01B1000cl\n",
      "N = 55 subjects\n",
      "\n",
      "-loading code version: csw1000block01.04.25.19\n",
      "N = 56 subjects\n",
      "\n",
      "-loading code version: RT40B1000cl\n",
      "N = 50 subjects\n",
      "\n",
      "-loading code version: csw1000block40.04.07.19\n",
      "N = 63 subjects\n"
     ]
    }
   ],
   "source": [
    "## load a thresholded dateset\n",
    "thresh = 0.9\n",
    "dfD = {}\n",
    "for cond in ALL_CONDITIONS:\n",
    "  dfD[cond] = load_final_df(cond,threshold=thresh) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## herlper for operating pandas dataframes\n",
    "compute_mean = lambda df_: df_.groupby(['story']).correct_response.mean()\n",
    "compute_stderr = lambda df_: df_.groupby(['story']).correct_response.std(\n",
    "  ) / np.sqrt(len(df_.index.get_level_values('subjnum').unique()))\n",
    "\n",
    "def plot_acc_with_background(df,ax=None,cL=['blue','green','blue','green','red']):\n",
    "  ## compute mean and stderr\n",
    "  M = compute_mean(df)\n",
    "  S = compute_stderr(df)\n",
    "  ## plot with error shading\n",
    "  if type(ax)==type(None):\n",
    "    ax = plt.gca()\n",
    "  ax.plot(M)\n",
    "  ax.fill_between(np.arange(len(M)),M-S,M+S,alpha=0.3)\n",
    "  ## make nice background\n",
    "  for idx in range(5):\n",
    "    ax.fill_between(np.arange(40*idx,41+40*idx),0,1.05,color=cL[idx],alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fg,axar = plt.subplots(len(dfD),1,figsize=(10,40),sharex=True);\n",
    "get_tstep_df = lambda tstep,df: df[df.index.get_level_values('depth') == tstep]\n",
    "cLD = {\n",
    "  'blocked': ['blue','green','blue','green','red'],\n",
    "  'blocked_rep': ['blue','green','blue','green','red'],\n",
    "  'interleaved': ['purple','purple','purple','purple','red'],\n",
    "  'interleaved_rep': ['purple','purple','purple','purple','red'],\n",
    "  'inserted_early': ['blue','green','purple','purple','red'],\n",
    "  'inserted_early_rep': ['blue','green','purple','purple','red'],\n",
    "  'inserted_middle': ['purple','blue','green','purple','red'],\n",
    "  'inserted_middle_rep': ['purple','blue','green','purple','red'],\n",
    "  'inserted_late': ['purple','purple','blue','green','red'],\n",
    "  'inserted_late_rep': ['purple','purple','blue','green','red'],\n",
    "  'explicit_interleaved': ['purple','purple','purple','purple','red'],\n",
    "}\n",
    "for idx,(c_str,c_df) in enumerate(dfD.items()):\n",
    "  ax = axar[idx]\n",
    "  ax.set_title(c_str)\n",
    "  ax.axhline(0.5,c='k',ls='--',lw=.3)\n",
    "  cL = cLD[c_str]\n",
    "  ## mean\n",
    "  plot_acc_with_background(dfD[c_str],cL=cL,ax=ax)\n",
    "  ## individual tsteps\n",
    "#   plot_acc_with_background(get_tstep_df(1,dfD[c_str]),cL=cL,ax=ax)\n",
    "#   plot_acc_with_background(get_tstep_df(2,dfD[c_str]),cL=cL,ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## between conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper for pulling mean test acc for each subject\n",
    "get_test_df = lambda df: df[df.index.get_level_values('block') == 4]\n",
    "## compute ttest\n",
    "def tstat_betweencond_testacc(df1,df2):\n",
    "  \"\"\" independent samples t-test\n",
    "  test acc of two conditions\n",
    "  \"\"\"\n",
    "  sub_testacc_df1 = get_test_df(df1).groupby(['subjnum']).correct_response.mean()\n",
    "  sub_testacc_df2 = get_test_df(df2).groupby(['subjnum']).correct_response.mean()\n",
    "  return ttest_ind(sub_testacc_df1,sub_testacc_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "big_str = \"\"\n",
    "for (c1,df1),(c2,df2) in itertools.product(dfD.items(),dfD.items()):\n",
    "  t,p = tstat_betweencond_testacc(df1,df2)\n",
    "  smal_str = \"%s vs %s\\n ttest_ind=%s, p=%s\"%(c1,c2,t,p)\n",
    "  print(smal_str)\n",
    "  big_str += \"\\n%s\"%smal_str\n",
    "\n",
    "with open('stats_reports/between_condition_test_acc.txt', 'w') as f:\n",
    "    f.write(big_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## within condition between timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helper for pulling df on specified tstep for each subject\n",
    "get_tstep_df = lambda tstep,df: df[df.index.get_level_values('depth') == tstep]\n",
    "# compute stat\n",
    "def tstat_wicond_betweenstep_testacc(df):\n",
    "  \"\"\" related samples t-test\n",
    "  test acc step 1 vs step 2\n",
    "  \"\"\"\n",
    "  # step 1\n",
    "  sub_testacc_step1 = get_tstep_df(1,get_test_df(df)\n",
    "                    ).groupby(['subjnum']).correct_response.mean()\n",
    "  # step 2\n",
    "  sub_testacc_step2 = get_tstep_df(2,get_test_df(df)\n",
    "                    ).groupby(['subjnum']).correct_response.mean()\n",
    "\n",
    "  return ttest_rel(sub_testacc_step1,sub_testacc_step2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_str2 = \"\"\n",
    "for cond_,df_ in dfD.items():\n",
    "  t,p = tstat_wicond_betweenstep_testacc(df_)\n",
    "  smal_str2 = \"%s step1-vs-step2 \\n ttest_rel=%s, p=%s\"%(cond_,t,p)\n",
    "  print(smal_str2)\n",
    "  big_str2 += \"\\n%s\"%smal_str2\n",
    "\n",
    "with open('stats_reports/within_condition_between_tstep_testacc.txt', 'w') as f:\n",
    "    f.write(big_str2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
